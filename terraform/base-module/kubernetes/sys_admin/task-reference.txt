#### Create User, Role, Rolebinding, & user.conf File
Creating a namespace
1. Create a namespace by using the following command:
    kubectl create namespace cap-cars-app

2. Create a directory role.
    mkdir roles
    cd roles

Generating an RSA private key and certificate requests
1. To generate an RSA private key, run the following command:
    sudo openssl genrsa -out cluster-admin.key 2048

2. Use the following command to generate certificate requests:
sudo openssl req -new -key cluster-admin.key -out cluster-admin.csr
    •	Organization Name: cap-cars-app
    •	Common Name: cluster-admin

3. Run the following command to link an identity to a private key using a digital signature.
    sudo openssl x509 -req -in cluster-admin.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key \
    -CAcreateserial -out cluster-admin.crt -days 500

Creating role
1. To create a role, add the following code to the cap-cars-admin.yml file.

    kind: Role
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
       namespace: cap-cars-app
       name: cluster-admin-role
    rules:
    - apiGroups: ["", "extensions", "apps"]
      resources: ["deployments", "pods", "services"]
      verbs: ["get", "list", "watch"]


2. Create a role by using the following command:
    kubectl create -f cap-cars-admin.yml -n cap-cars-app
    kubectl get roles -n cap-cars-app

Creating a rolebinding
1. To create a rolebinding, add the following code to the cap-cars-admin-rb.yml file.

    kind: RoleBinding
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
     name: cluster-admin-rb
     namespace: role
    subjects:
    - kind: User
      name: cluster-admin
      apiGroup: ""
    roleRef:
      kind: Role
      name: cluster-admin-role
      apiGroup: ""

2. Create rolebinding by using the following command:
    kubectl create -f cap-cars-admin-rb.yml
    kubectl get rolebinding -n role

Setting credentials to the user
1. Set credentials to cluster-admin.
    kubectl config set-credentials cluster-admin --client-certificate=/home/labsuser/roles/cluster-admin.crt \
    --client-key=/home/labsuser/roles/cluster-admin.key

2. Set context to cluster-admin.
    kubectl config set-context cluster-admin-context --cluster=kubernetes --namespace=cap-cars-app --user=cluster-admin

3. Run the following command to display current contexts:
    kubectl config get-contexts

Copying the config file to the client machine
1. Copy the config file from the master node in the home directory.
    cd ..
    cat .kube/config

2. Paste the copied config file into the client machine.
    vim cluster-admin.conf

3. Copy the crt and key files from the master node to the client node in the /role directory.
    mkdir roles
    cd role
    vim cluster-admin.crt
	vim cluster-admin.key

Verifying roles
1. Locate the home directory.
    cd ..

2. Run the following commands to verify roles we have generated:
    kubectl get pods --kubeconfig=cluster-admin.conf
    kubectl create deployment test --image=docker.io/httpd -n role --kubeconfig=cluster-admin.conf
    kubectl get pods --kubeconfig=cluster-admin.conf
    kubectl get deployment --kubeconfig=cluster-admin.conf

The worker node can create, update, remove, and list pods, services, and deployments after using the master config \
settings.


#### Back up etcd cluster data
1. Install the etcd-client:
    sudo apt install etcd-client
2. List all the pods of the kube-system namespace
    kubectl get pods -n kube-system
3. Describe the etcd pod of the kube-system namespace and copy the IP address of the --advertise-client-url flag:
    kubectl describe pods <etcd-pod-name> -n kube-system
4. Export the advertise-client-url to advertise_url
    export advertise_url=<<advertise-client-url>>
    echo $advertise_url
5. Use the following command to save the etcd backup:
    sudo ETCDCTL_API=3 etcdctl \
    --endpoints $advertise_url \
    --cacert /etc/kubernetes/pki/etcd/ca.crt \
    --key /etc/kubernetes/pki/etcd/server.key \
    --cert /etc/kubernetes/pki/etcd/server.crt snapshot save etcd_backup.db
6. Use the following command to check the newly created etcd_backup.db file:
    ls






